#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´ä¼˜åŒ–è®­ç»ƒè„šæœ¬ - å±•ç¤ºMOEæ¨¡å‹çš„æ‰€æœ‰æ”¹è¿›
"""

import torch
import torch.nn as nn
import pytorch_lightning as pl
from torch.utils.data import DataLoader, Dataset
from optimized_moe import OptimizedMOE
import numpy as np

class TrajectoryDataset(Dataset):
    """è½¨è¿¹é¢„æµ‹æ•°æ®é›†"""
    
    def __init__(self, size=100, seq_len=20, num_agents=30, feature_dim=32):
        self.size = size
        self.seq_len = seq_len
        self.num_agents = num_agents
        self.feature_dim = feature_dim
        
    def __len__(self):
        return self.size
    
    def __getitem__(self, idx):
        # åˆ›å»ºæ¨¡æ‹Ÿè½¨è¿¹æ•°æ®
        obj_trajs = torch.randn(self.num_agents, self.seq_len, self.feature_dim)
        obj_trajs_mask = torch.ones(self.num_agents, self.seq_len)
        
        # éšæœºé®è”½ä¸€äº›ä»£ç†
        mask_indices = torch.rand(self.num_agents) > 0.7
        obj_trajs_mask[mask_indices] = 0
        
        input_dict = {
            'obj_trajs': obj_trajs,
            'obj_trajs_mask': obj_trajs_mask,
            'map_polylines': torch.randn(100, 20, 64),
            'map_polylines_mask': torch.ones(100, 20),
            'track_index_to_predict': torch.randint(0, self.num_agents, (1,)),
            'center_gt_trajs': torch.randn(60, 2),
            'center_gt_trajs_mask': torch.ones(60),
            'center_gt_final_valid_idx': torch.randint(0, 60, (1,)),
            'center_objects_world': torch.randn(7),
            'map_center': torch.randn(2),
            'scenario_id': f"scenario_{idx}",
            'center_objects_id': f"object_{idx}",
            'center_objects_type': torch.randint(1, 4, (1,)),
            'dataset_name': "dummy"
        }
        
        return {'input_dict': input_dict}
    
    def collate_fn(self, batch):
        """æ‰¹å¤„ç†å‡½æ•°"""
        batched_input_dict = {}
        keys = batch[0]['input_dict'].keys()
        
        for key in keys:
            values = [sample['input_dict'][key] for sample in batch]
            if isinstance(values[0], torch.Tensor):
                batched_input_dict[key] = torch.stack(values)
            else:
                batched_input_dict[key] = values
        
        return {
            'batch_size': len(batch),
            'input_dict': batched_input_dict
        }

class OptimizedMOETrainingModule(pl.LightningModule):
    """ä¼˜åŒ–ç‰ˆMOEè®­ç»ƒæ¨¡å—"""
    
    def __init__(self, model, learning_rate=0.001):
        super().__init__()
        self.model = model
        self.learning_rate = learning_rate
        self.loss_fn = nn.MSELoss()
        
    def forward(self, x):
        return self.model(x)
    
    def training_step(self, batch, batch_idx):
        # MOEæ¨¡å‹å‰å‘ä¼ æ’­
        prediction, total_loss, routing_probs = self.model(batch)
        
        # è®°å½•æŸå¤±å’Œè·¯ç”±ç»Ÿè®¡ä¿¡æ¯
        self.log('train_total_loss', total_loss, on_step=True, on_epoch=True)
        
        # è®°å½•è·¯ç”±ç»Ÿè®¡ä¿¡æ¯
        stats = self.model.get_routing_statistics(routing_probs)
        self.log('train_expert_usage_var', stats['usage_variance'], on_step=False, on_epoch=True)
        self.log('train_max_expert_usage', stats['max_usage'], on_step=False, on_epoch=True)
        self.log('train_min_expert_usage', stats['min_usage'], on_step=False, on_epoch=True)
        
        return total_loss
    
    def validation_step(self, batch, batch_idx):
        # MOEæ¨¡å‹å‰å‘ä¼ æ’­
        prediction, total_loss, routing_probs = self.model(batch)
        
        # è®°å½•æŸå¤±å’Œè·¯ç”±ç»Ÿè®¡ä¿¡æ¯
        self.log('val_total_loss', total_loss, on_step=False, on_epoch=True)
        
        # è®°å½•è·¯ç”±ç»Ÿè®¡ä¿¡æ¯
        stats = self.model.get_routing_statistics(routing_probs)
        self.log('val_expert_usage_var', stats['usage_variance'], on_step=False, on_epoch=True)
        self.log('val_max_expert_usage', stats['max_usage'], on_step=False, on_epoch=True)
        self.log('val_min_expert_usage', stats['min_usage'], on_step=False, on_epoch=True)
        
        return total_loss
    
    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)
        return [optimizer], [scheduler]

def run_optimization_demo():
    """è¿è¡Œä¼˜åŒ–æ¼”ç¤º"""
    print("ğŸš€ å¼€å§‹MOEæ¨¡å‹å®Œæ•´ä¼˜åŒ–æ¼”ç¤º...")
    
    # é…ç½®å‚æ•°
    config = {
        'input_dim': 32,
        'num_experts': 3,  # å¢åŠ ä¸“å®¶æ•°é‡
        'hidden_dim': 64,
        'output_dim': 32
    }
    
    # åˆ›å»ºä¼˜åŒ–ç‰ˆMOEæ¨¡å‹
    print("ğŸ”§ åˆ›å»ºä¼˜åŒ–ç‰ˆMOEæ¨¡å‹...")
    moe_model = OptimizedMOE(config)
    
    # åˆ›å»ºè®­ç»ƒæ¨¡å—
    training_module = OptimizedMOETrainingModule(moe_model, learning_rate=0.001)
    
    # åˆ›å»ºæ•°æ®é›†
    print("ğŸ“¦ åˆ›å»ºæ•°æ®é›†...")
    train_dataset = TrajectoryDataset(200)  # å¢åŠ è®­ç»ƒæ•°æ®é‡
    val_dataset = TrajectoryDataset(40)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, 
        batch_size=8,  # å¢åŠ æ‰¹æ¬¡å¤§å°
        shuffle=True, 
        collate_fn=train_dataset.collate_fn,
        num_workers=0  # é¿å…Windowså¤šè¿›ç¨‹é—®é¢˜
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=8, 
        shuffle=False, 
        collate_fn=val_dataset.collate_fn,
        num_workers=0
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("ğŸƒ åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = pl.Trainer(
        max_epochs=5,  # å¢åŠ è®­ç»ƒè½®æ•°
        accelerator="cpu",  # ä½¿ç”¨CPUè®­ç»ƒ
        devices=1,
        logger=False,  # ç¦ç”¨æ—¥å¿—è®°å½•
        enable_checkpointing=False  # ç¦ç”¨æ£€æŸ¥ç‚¹
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("ğŸ”¥ å¼€å§‹è®­ç»ƒ...")
    trainer.fit(
        model=training_module,
        train_dataloaders=train_loader,
        val_dataloaders=val_loader
    )
    
    # å±•ç¤ºæœ€ç»ˆçš„è·¯ç”±ç»Ÿè®¡ä¿¡æ¯
    print("ğŸ“Š æœ€ç»ˆè·¯ç”±ç»Ÿè®¡ä¿¡æ¯:")
    # åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ‰¹æ¬¡è¿›è¡Œå‰å‘ä¼ æ’­
    sample_batch = next(iter(val_loader))
    _, _, routing_probs = moe_model(sample_batch)
    final_stats = moe_model.get_routing_statistics(routing_probs)
    
    print(f"   ä¸“å®¶ä½¿ç”¨æ–¹å·®: {final_stats['usage_variance'].item():.6f}")
    print(f"   æœ€å¤§ä¸“å®¶ä½¿ç”¨ç‡: {final_stats['max_usage'].item():.4f}")
    print(f"   æœ€å°ä¸“å®¶ä½¿ç”¨ç‡: {final_stats['min_usage'].item():.4f}")
    print(f"   ä¸“å®¶ä½¿ç”¨ç‡: {final_stats['expert_usage']}")
    
    print("ğŸ‰ MOEæ¨¡å‹å®Œæ•´ä¼˜åŒ–æ¼”ç¤ºå®Œæˆ!")

def compare_with_baseline():
    """ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”"""
    print("\nğŸ” ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”...")
    
    # åŸºçº¿é…ç½®ï¼ˆ2ä¸ªä¸“å®¶ï¼‰
    baseline_config = {
        'input_dim': 32,
        'num_experts': 2,
        'hidden_dim': 64,
        'output_dim': 32
    }
    
    # ä¼˜åŒ–é…ç½®ï¼ˆ3ä¸ªä¸“å®¶ï¼‰
    optimized_config = {
        'input_dim': 32,
        'num_experts': 3,
        'hidden_dim': 64,
        'output_dim': 32
    }
    
    # åˆ›å»ºæ¨¡å‹
    baseline_model = OptimizedMOE(baseline_config)
    optimized_model = OptimizedMOE(optimized_config)
    
    # åˆ›å»ºç›¸åŒè¾“å…¥
    batch_size = 4
    input_dict = {
        'input_dict': {
            'obj_trajs': torch.randn(batch_size, 30, 20, 32)
        }
    }
    
    # å‰å‘ä¼ æ’­
    _, baseline_loss, baseline_routing = baseline_model(input_dict)
    _, optimized_loss, optimized_routing = optimized_model(input_dict)
    
    # æ¯”è¾ƒç»“æœ
    print(f"   åŸºçº¿æ¨¡å‹æŸå¤±: {baseline_loss.item():.4f}")
    print(f"   ä¼˜åŒ–æ¨¡å‹æŸå¤±: {optimized_loss.item():.4f}")
    print(f"   æŸå¤±æ”¹å–„: {baseline_loss.item() - optimized_loss.item():.4f}")
    
    # æ¯”è¾ƒè·¯ç”±ç»Ÿè®¡
    baseline_stats = baseline_model.get_routing_statistics(baseline_routing)
    optimized_stats = optimized_model.get_routing_statistics(optimized_routing)
    
    print(f"   åŸºçº¿ä¸“å®¶ä½¿ç”¨æ–¹å·®: {baseline_stats['usage_variance'].item():.6f}")
    print(f"   ä¼˜åŒ–ä¸“å®¶ä½¿ç”¨æ–¹å·®: {optimized_stats['usage_variance'].item():.6f}")
    print(f"   è´Ÿè½½å¹³è¡¡æ”¹å–„: {baseline_stats['usage_variance'].item() - optimized_stats['usage_variance'].item():.6f}")
    
    print("âœ… å¯¹æ¯”å®Œæˆ!")

if __name__ == "__main__":
    # è¿è¡Œä¼˜åŒ–æ¼”ç¤º
    run_optimization_demo()
    
    # ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”
    compare_with_baseline()